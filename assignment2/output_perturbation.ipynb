{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Private Training by Output Perturbation.\"\"\"\n",
    "import argparse\n",
    "\n",
    "import numpy as np\n",
    "from scipy.stats import ortho_group\n",
    "import torch\n",
    "from torch.distributions.gamma import Gamma\n",
    "from torch import nn\n",
    "\n",
    "from logistic_regression import nonprivate_logistic_regression\n",
    "from utils import get_data_loaders"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gamma_sample_pytorch_parameterization(concentration, rate):\n",
    "    \"\"\"The Gamma dist'n as it is parameterized in PyTorch\"\"\"\n",
    "    return Gamma(concentration, rate).sample()\n",
    "\n",
    "\n",
    "def gamma_sample_chaudhuri_parameterization(concentration, scale):\n",
    "    \"\"\"The Gamma dist'n as it is parameterized in Chaudhuri and Monteleoni\"\"\"\n",
    "    rate = 1. / scale\n",
    "    return gamma_sample_pytorch_parameterization(concentration, rate)\n",
    "\n",
    "\n",
    "def random_unit_norm_vector(num_dims):\n",
    "    random_rotation_matrix = ortho_group.rvs(num_dims)\n",
    "    basis_vector_one = np.eye(num_dims)[0]\n",
    "    vector = np.matmul(random_rotation_matrix, basis_vector_one)\n",
    "    return torch.tensor(vector, dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "your code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def private_logistic_regression(dset_loader, num_epochs, learning_rate,\n",
    "    lmbda, epsilon, seed=None):\n",
    "    ############################################################################\n",
    "    # TODO(student)\n",
    "    #\n",
    "    # your code here...\n",
    "    #\n",
    "    # hint: use the code we have given you. For example you don't have to \n",
    "    # implement non-private logistic regression from scratch because an \n",
    "    # implementation exists in logistic_regression.py. There are also functions \n",
    "    # in this file for sampling Laplace noise\n",
    "    #\n",
    "    # hint: the input dim d can be found as a attr of the dset_loader's dset\n",
    "    #       >>> num_pixels = dset_loader.dataset.num_pixels\n",
    "    #\n",
    "    private_params = {\n",
    "        'weight': torch.tensor([-1.]),  # replace me (but this is how to format the state_dict)\n",
    "        }\n",
    "    raise NotImplementedError\n",
    "    ############################################################################\n",
    "\n",
    "    \n",
    "    return private_params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "main function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(n, epsilon, lmbda, epochs, batch_size, lr, data_seed, model_seed):\n",
    "    # load data\n",
    "    loaders, _ = get_data_loaders(data_seed, batch_size, n)\n",
    "    loaders.pop('neighbor')  # don't need this loader for this question\n",
    "  \n",
    "    # train model\n",
    "    nonprivate_params = \\\n",
    "            nonprivate_logistic_regression(loaders['train'], epochs, \n",
    "                    lr, lmbda, seed=model_seed)\n",
    "  \n",
    "    private_params = private_logistic_regression(loaders['train'], epochs, \n",
    "        lr, lmbda, epsilon, seed=model_seed)\n",
    "  \n",
    "    # evaluate\n",
    "    test_losses = dict()\n",
    "    test_accs = dict()\n",
    "    for name, params in zip(['nonprivate', 'private'], \n",
    "          [nonprivate_params, private_params]):\n",
    "        num_pixels = loaders['train'].dataset.num_pixels\n",
    "        model = nn.Linear(num_pixels, 1, bias=False)\n",
    "        criterion = nn.BCEWithLogitsLoss()  # binary cross entropy\n",
    "        model.load_state_dict(params)\n",
    "        model.eval()\n",
    "        num_test_examples = len(loaders['test'].dataset)\n",
    "        with torch.no_grad():\n",
    "            test_loss = 0.\n",
    "            correct = 0\n",
    "            total = 0\n",
    "            for images, labels in loaders['test']:\n",
    "                images = images.reshape(-1, 28*28)\n",
    "                outputs = model(images)\n",
    "                loss = criterion(outputs.squeeze(), labels.float())\n",
    "                test_loss += loss.item() * len(images) / float(num_test_examples)\n",
    "                predicted = (outputs.squeeze() > 0.).long()\n",
    "                total += labels.size(0)\n",
    "                correct += (predicted == labels).sum().item()\n",
    "            test_acc = float(correct) / float(total)\n",
    "            test_losses[name] = test_loss\n",
    "            test_accs[name] = 100. * test_acc  # format as a percentage\n",
    "  \n",
    "    from pprint import pprint\n",
    "    print('final test losses')\n",
    "    print('nonprivate: {nonprivate:.2f}, private: {private:.2f}'\n",
    "          .format(**test_losses))\n",
    "    print('final test accs')\n",
    "    print('nonprivate: {nonprivate:.2f}, private: {private:.2f}'\n",
    "          .format(**test_accs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "arguments and main function call"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "49e9c3ec09f9453aaecccc3006d1964f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=10), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "NotImplementedError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNotImplementedError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-36ffbbcc73bb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mMODEL_SEED\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mN\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mEPSILON\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mLMBDA\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mEPOCHS\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mBATCH_SIZE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mLR\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDATA_SEED\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mMODEL_SEED\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-8-ef42fdd6f247>\u001b[0m in \u001b[0;36mmain\u001b[0;34m(n, epsilon, lmbda, epochs, batch_size, lr, data_seed, model_seed)\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     private_params = private_logistic_regression(loaders['train'], epochs, \n\u001b[0;32m---> 12\u001b[0;31m         lr, lmbda, epsilon, seed=model_seed)\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0;31m# evaluate\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-7-ebf34abf9242>\u001b[0m in \u001b[0;36mprivate_logistic_regression\u001b[0;34m(dset_loader, num_epochs, learning_rate, lmbda, epsilon, seed)\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0;34m'weight'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1.\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# replace me (but this is how to format the state_dict)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m         }\n\u001b[0;32m---> 19\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mNotImplementedError\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m     \u001b[0;31m############################################################################\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNotImplementedError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "N = 1000\n",
    "EPSILON = 2.\n",
    "LMBDA = 5e-4\n",
    "EPOCHS = 10  # run for more epochs once your code works\n",
    "BATCH_SIZE = 256\n",
    "LR = .1\n",
    "DATA_SEED = 0\n",
    "MODEL_SEED = 0\n",
    "\n",
    "main(N, EPSILON, LMBDA, EPOCHS, BATCH_SIZE, LR, DATA_SEED, MODEL_SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
